---
title: "SDTEF - Homelessness Program Analysis"
author: "Brandon Miner"
date: "`r Sys.Date()`"
output: pdf_document
---

# SDCTA - RTFH Analysis

## Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(ggcorrplot)
library(glmnet)
library(lme4)
library(lmerTest)
library(pander)
library(broom)
library(kableExtra)

dat = read.csv('../data/processed/pivoted_and_PIT.csv')

head(dat)

# Sample data frame

standardize = function(col){
  col = (col - mean(col)) / sd(col)
  return(col)
}

dat_standard = dat |>
  mutate(across(where(is.numeric) & !all_of("Unsheltered.Per.100.000"), standardize))

```

## OLS Regression

```{r correlation_check}
ignored_features = c("PEH.Per.100.000", "Total.PEH", "Unsheltered.PEH", "Population")

corr <- round(cor(
  dat |>
    select(-all_of(c(ignored_features,"City")))
), 1)

ggcorrplot(corr, hc.order = TRUE, type = "upper", outline.col = "white", lab = FALSE)
```

```{r linreg}
ignored_features = c("PEH.Per.100.000", "Total.PEH", "Unsheltered.PEH", "Population")

model = lm(Unsheltered.Per.100.000 ~ ., data = dat |> select(-all_of(ignored_features)))
AIC(model)

modelS = lm(Unsheltered.Per.100.000 ~ ., data = dat_standard |> select(-all_of(ignored_features)))
AIC(modelS)

```

```{r coeff_analysis}
coef_values <- coef(model)

# Rank coefficients by absolute value, in descending order
ranked_coef <- sort(coef_values, decreasing = TRUE)

# View the ranked coefficients
l = length(ranked_coef)
print(formatC(ranked_coef, format = "f", digits = 4))

print("##################################################################################################################################################################")

coef_values <- coef(modelS)

# Rank coefficients by absolute value, in descending order
ranked_coefS <- sort(coef_values, decreasing = TRUE)

# View the ranked coefficients
print(formatC(ranked_coefS, format = "f", digits = 4))
```

```{r top_and_bottom_3}
print("For each additional dollar put towards the below programs, PEH Per 100,000 increases by the listed amount")
print(formatC(ranked_coef[c(6:8, 27:29)], format = "f", digits = 4))
print("")
print("")
print("---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------")
print("")
print("")
print("For each standard deviation increase in the dollar amount spent towards the below programs, PEH Per 100,000 increases by the listed amount")
print(formatC(ranked_coefS[c(2:4,36:38)], format = "f", digits = 4))

```

## LOESS Regression is only for a small number of predictors/features ($<5$)

## LASSO Regression

LASSO Regression also makes little sense because we specifically *want* coefficients for all features, namely all the programs, and their coeffs are reduced to zero. Additionally LASSO adds sparsity to our dataset, and our dataset is already sparse. Below is the LASSO model to show it's lack of helpfulness.

Since we want an "accurate enough" model to identify trends, linear regression is all we need.

```{r LASSOreg}
# Convert predictors to a matrix
x <- dat |>
  select(-all_of(c(ignored_features, "Unsheltered.Per.100.000"))) |>
  select(where(is.numeric)) |>
  as.matrix()

xS <- dat_standard |>
  select(-all_of(c(ignored_features, "Unsheltered.Per.100.000"))) |>
  select(where(is.numeric)) |>
  as.matrix()

# Define the response variable
y <- dat$PEH.Per.100.000  # Replace with your actual response variable


set.seed(123)
cv_model <- cv.glmnet(x, y, alpha = 1)
cv_modelS <- cv.glmnet(xS, y, alpha = 1)

# Find the best lambda value
best_lambda <- cv_model$lambda.min
best_lambdaS <- cv_modelS$lambda.min

# Get the coefficients for the best lambda
lasso_coefs <- coef(cv_model, s = "lambda.min")
lasso_coefsS <- coef(cv_modelS, s = "lambda.min")

print(lasso_coefs)
print(lasso_coefsS)
```

## Mixed Effects Model

### Implement Models

```{r}
ignored_features = c("PEH.Per.100.000", "Total.PEH", "Unsheltered.PEH", "Population")


model <- lmer(Unsheltered.Per.100.000 ~ 
              bridge.to.housing.network + emergency.shelter + family.reunification.program + 
              flexible.funds + food.and.nutrition + homeless.services + homelessness.prevention + 
              homeshare.program + housing.assistance + housing.navigation.services + 
              housing.stability.services + motel.voucher + neighborhood.revitalization.services + 
              opening.doors.program + outreach + project.h.o.p.e. + rapid.re.housing + 
              rental.assistance + restrooms + safe.parking + service.center + staff.and.operations + 
              take.back.the.streets + transitional.housing + work.for.hope + 
              (1 + Year | City), data = dat |> select(-all_of(ignored_features)))

modelS <- lmer(Unsheltered.Per.100.000 ~ 
              bridge.to.housing.network + emergency.shelter + family.reunification.program + 
              flexible.funds + food.and.nutrition + homeless.services + homelessness.prevention + 
              homeshare.program + housing.assistance + housing.navigation.services + 
              housing.stability.services + motel.voucher + neighborhood.revitalization.services + 
              opening.doors.program + outreach + project.h.o.p.e. + rapid.re.housing + 
              rental.assistance + restrooms + safe.parking + service.center + staff.and.operations + 
              take.back.the.streets + transitional.housing + work.for.hope + 
              (1 + Year | City), data = dat_standard |> select(-all_of(ignored_features)))
```

### Model Summaries

```{r}
summarize_model <- function(model) {
  # Extract the full model summary
  model_summary <- summary(model)
  
  # Convert the fixed effects table to a data frame
  fixed_effects <- as.data.frame(model_summary$coefficients)
  
  # Sort the fixed effects by Estimate
  sorted_fixed_effects <- fixed_effects[order(fixed_effects$Estimate), ]
  
  # Add the C-like format for estimates, but remove the predictor names in the formatted column
  sorted_fixed_effects$Estimates <- sprintf("%.4f", sorted_fixed_effects$Estimate)
  
  # Remove the original 'Estimate' column
  sorted_fixed_effects <- sorted_fixed_effects[, !names(sorted_fixed_effects) %in% c("Estimate")]
  
  # Rearrange the columns to move 'Estimates' to the front
  sorted_fixed_effects <- sorted_fixed_effects[, c("Estimates", setdiff(names(sorted_fixed_effects), "Estimates"))]
  
  # Print the modified summary with formatted estimates using asis_output for proper rendering
  sorted_fixed_effects %>%
    kable("latex", booktabs = TRUE) %>%
    kable_styling(latex_options = "scale_down")
}

summarize_model(model)
summarize_model(modelS)

```

### Plot Estimates

```{r}
# Extract fixed effects
fixed_effects <- as.data.frame(summary(modelS)$coefficients)

# Remove the intercept row
fixed_effects_no_intercept <- fixed_effects[rownames(fixed_effects) != "(Intercept)", ]

# Reorder the predictors based on the Estimate
fixed_effects_no_intercept$Predictor <- factor(
  rownames(fixed_effects_no_intercept),
  levels = rownames(fixed_effects_no_intercept)[order(fixed_effects_no_intercept$Estimate)]
)

# Plot the fixed effects excluding the intercept, sorted by Estimate
ggplot(fixed_effects_no_intercept, aes(x = Predictor, y = Estimate)) +
  geom_bar(stat = "identity") +
  theme(
    axis.text.x = element_text(angle = 60, hjust = 1, size = 10), # Rotate and adjust the labels
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  ) +
  labs(
    title = "Fixed Effects Coefficients",
    x = "Predictors",
    y = "Estimate"
  )
```


### Mixed Effects Model Assuptions

#### 1. Linearity
The is a necessary assumption, but it could be argued that we want "general trend" so linearity is not a horrible solution.

```{r linearity_test}
plot(model, which = 1)
```

Technically would want to check against each City and each Year but our dataset is not built for that.

#### 2. Independence of Errors
Difficult to prove

#### 3. Homoscedasticity
We must assume that the variance of PEH is constant. This sounds like a t-test 

#### 4. Normality of Random Effects and Residuals
Assume the amount spent on each program is normally distributed.

```{r Q-Q_rand_eff}
# Plot residuals
for (i in 8:ncol(dat_standard)) {
  # Extract the column name for labeling
  col_name <- names(dat_standard)[i]
  
  # Plot the residuals for the current column
  qqnorm(dat_standard[[i]], main = paste("Q-Q Plot of", col_name))
  qqline(dat_standard[[i]])
}

```

Almost all programs are approximately normal. There are exceptions, but concerns of losing more data is real.

The Q-Q plot shows some concerns due to is not being clearly normal

```{r Q-Q_resid}
# Plot residuals
plot(resid(model))
qqnorm(resid(model))
qqline(resid(model))
```

#### 5. Correct Specification of Random Effects
We can say with confidence that Year and City are fixed effects while the programs are to be considered random effects.

#### 6. Independence between Fixed Effects and Random Effects
Cities is a categorical value so it is easily prone to multicollinearity. This is an unfortunate fact that we has t ocheck and address.

#### 7. Random Effects are Correctly Modeled as Random
Same as .5 for the most part.

#### 8. No Overfitting
Our data is population data so there is technically no fear of overfitting unless you consider all expenditures/programs prior to 2015.













