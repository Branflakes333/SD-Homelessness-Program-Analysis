{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDCTA - RTFH Project\n",
    "\n",
    "This script contains all data cleaning scripts and processes from raw to processed data. Each sction is divided by output CSV file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline\n",
    "raw datasets:\n",
    "- `CityExpendituresRaw.csv`\n",
    "- `PITCount.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `processed.csv`\n",
    "Input dataset:\n",
    "- `CityExpendituresRaw.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "raw = pd.read_csv(\"../data/raw/CityExpendituresRaw.csv\")\n",
    "raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Filtering**\n",
    "\n",
    "Since the goal is to filter the unique programs down as small as possible with the intention of combinining programs that do the same thing, but named slightly differently, checking if any of the `Program` row strings contain the name of another program could yield some level of shrinkage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update program names by checking for occurrences in the 'Program' column\n",
    "def update_program_column(processed):\n",
    "    \"\"\"\n",
    "    Update program names by checking if other rows contain the same string.\n",
    "\n",
    "    Parameters:\n",
    "    - processed: The DataFrame containing the 'Program' column.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame with updated program names.\n",
    "    \"\"\"\n",
    "    # Create a mapping of unique program values\n",
    "    program_mapping = {val: val for val in processed['Program'].dropna().unique()}\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in processed.iterrows():\n",
    "        program = row['Program']\n",
    "        # Check if the program is a string\n",
    "        if isinstance(program, str):\n",
    "            # Update other occurrences of the program name\n",
    "            processed.loc[processed['Program'].str.contains(program, na=False), 'Program'] = program_mapping[program]\n",
    "\n",
    "    return processed\n",
    "\n",
    "\n",
    "# Function to replace dashes with spaces in the 'Program' column\n",
    "def replace_dashes_with_spaces(processed):\n",
    "    \"\"\"\n",
    "    Replace all dashes in the 'Program' column with spaces.\n",
    "\n",
    "    Parameters:\n",
    "    - processed: The DataFrame containing the 'Program' column.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame with the dashes replaced by spaces.\n",
    "    \"\"\"\n",
    "    # Replace dashes with spaces in the 'Program' column\n",
    "    processed['Program'] = processed['Program'].str.replace('-', ' ', regex=False)\n",
    "\n",
    "    return processed\n",
    "\n",
    "# Create copy of raw data frame for data cleaing\n",
    "processed = raw.copy()\n",
    "\n",
    "# Convert all program names to lowercase for standardization\n",
    "processed['Program'] = processed['Program'].str.lower()\n",
    "\n",
    "# Replace dashes with spaces in the 'Program' column\n",
    "processed = replace_dashes_with_spaces(processed)\n",
    "\n",
    "# Update program names based on occurrences in the DataFrame\n",
    "processed = update_program_column(processed)\n",
    "\n",
    "# Display the number of unique program names before and after processing\n",
    "print(\"Unique program names in raw data:\", len(raw[\"Program\"].unique()))\n",
    "print(\"Unique program names in processed data:\", len(processed['Program'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual cleaning Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_program_value(processed, value_to_convert, new_value):\n",
    "    \"\"\"\n",
    "    Convert a specified value in the 'Program' column of the processed DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - processed: The DataFrame containing the 'Program' column.\n",
    "    - value_to_convert: The value in the 'Program' column that you want to convert.\n",
    "    - new_value: The new value to replace the old value with.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame with the specified conversion applied.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace the specified value in the 'Program' column\n",
    "    processed['Program'] = processed['Program'].replace(value_to_convert, new_value)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = convert_program_value(processed,\n",
    "                                    'address homeless issues through case management; provide food, shelter vouchers, as well as skill development for long-term self-sufficiency to 200 residents.',  \n",
    "                                   'address homeless issues through case management, provide food, shelter vouchers, and skill development for long-term self sufficiency'\n",
    ")\n",
    "\n",
    "processed = convert_program_value(processed,\n",
    "                                  'outreeach',\n",
    "                                  'outreach'\n",
    ")\n",
    "print(\"Unique program names in processed data:\", processed['Program'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the data cleaning from which `processed.csv` is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'data/processed' directory if it doesn't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join('data', 'processed', 'processed.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "processed.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `updated_dataset.csv`\n",
    "Input dataset:\n",
    "- `processed.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/processed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_program_names = df['Program'].unique()\n",
    "print(unique_program_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_mapping = {\n",
    "    'rapid rehousing': 'rapid rehousing program',\n",
    "    'nousing navigation': 'housing navigation services',\n",
    "    'provide services to families, abused youth, seniors and veterans experiencing homelessness and domestic violence with housing and wrap around services.': 'flexible funds',\n",
    "    'homless prevention plan': 'homeless prevention',\n",
    "    'provide support for regional homeless service providers, networking and communication for organizations serving and impacted by homeless persons, and building capacity of the east county homeless task force': 'flexible funds',\n",
    "    'motel program': 'motel voucher',\n",
    "    'navigation center': 'housing navigation services',\n",
    "    'hsg/econ dev homeless brochure': 'homelessness educational initiatives',\n",
    "    'postage homelessness education': 'homelessness educational initiatives',\n",
    "    'homelessness education mailer': 'homelessness educational initiatives',\n",
    "    'think dignity tsc manual': 'homelessness educational initiatives',\n",
    "    'homeless ed postcard design': 'homelessness educational initiatives',\n",
    "}\n",
    "df['Program'] = df['Program'].str.lower().str.strip()\n",
    "df['Program'] = df['Program'].replace(program_mapping)\n",
    "\n",
    "print(df['Program'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_mapping = {\n",
    "    'homelessness prevenetion': 'homelessness prevention', # fixed typo\n",
    "    'provide emergency housing to imminently homeless, or episodically and chronically homeless individuals and families in the city of santee, and who are unable to access housing during the coronavirus pandemic': 'flexible funds',\n",
    "    'homeless prevention and intervention': 'homlessness prevention and intervention',\n",
    "    'rapid rehousing program': 'rapid re housing',\n",
    "    'homlessness services': 'homeless services',\n",
    "    'litter removal': 'neighborhood revitalization services',\n",
    "    'emcampment/trash cleanup': 'neighborhood revitalization services',\n",
    "    'facility imporvement': 'facility improvement', #fixed typo\n",
    "    'hvac replacements': 'facility improvement',\n",
    "    'improve fencing': 'railing/fencing improvements',\n",
    "    'railing replacement': 'railing/fencing improvements',\n",
    "    'security fencing': 'railing/fencing improvements',\n",
    "}\n",
    "df['Program'] = df['Program'].str.lower().str.strip()\n",
    "df['Program'] = df['Program'].replace(program_mapping)\n",
    "\n",
    "print(df['Program'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_mapping = {\n",
    "    'rapid rehousing services': 'rapid rehousing',\n",
    "    'homeshare for seniros': 'homeshare for seniors', # typo\n",
    "    'cortez hill family center interim housing program': 'interim housing facility',\n",
    "    'homeshare for seniors': 'homeshare program',\n",
    "    'encampment/trash cleanup': 'neighborhood revitalization services',\n",
    "    'permanent housing with supportive services and property management': 'fair housing',\n",
    "    'homlessness prevention and intervention': 'homelessness prevention',\n",
    "    'general homelessness services': 'homeless services',\n",
    "    'homeless storage check in center': 'transitional storage',\n",
    "}\n",
    "df['Program'] = df['Program'].str.lower().str.strip()\n",
    "df['Program'] = df['Program'].replace(program_mapping)\n",
    "\n",
    "print(df['Program'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_mapping = {\n",
    "    'homeshare for seniors': 'homeshare program',\n",
    "    'rapid rehousing': 'rapid re housing',\n",
    "    'afforable housing fund services': 'housing assistance',\n",
    "    'prevention programs': 'homeless prevention',\n",
    "    'food drive services': 'food and nutrition',\n",
    "    'emergency food supplies': 'food and nutrition',\n",
    "    'bridge housing transitional housing': 'transitional housing and supportive services',\n",
    "    'turning point transitional living program': 'transitional housing and supportive services',\n",
    "}\n",
    "df['Program'] = df['Program'].str.lower().str.strip()\n",
    "df['Program'] = df['Program'].replace(program_mapping)\n",
    "\n",
    "print(df['Program'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_mapping = {\n",
    "    'facility improvements': 'facility improvement',\n",
    "    'scattered site': 'shelter',\n",
    "    'restroom rental': 'restrooms', # not sure if these need to be combined but they seem similar\n",
    "    'homelessness response center services': 'service center',\n",
    "    'operation of city of san diego day center for homeless adults': 'service center',\n",
    "    'supportive service  a way back home': 'family reunification program',\n",
    "    'safetay network program': 'outreach',\n",
    "}\n",
    "df['Program'] = df['Program'].str.lower().str.strip()\n",
    "df['Program'] = df['Program'].replace(program_mapping)\n",
    "\n",
    "print(df['Program'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_mapping = {\n",
    "    'hygiene supplies': 'homeless services',\n",
    "    'city housing support': 'housing assistance',\n",
    "    'animal care': 'homeless services',\n",
    "    'low income housing services': 'homeless services',\n",
    "    'interprovider networking and program facilitation': 'flexible funds',\n",
    "    'homeless prevention program': 'homeless prevention',\n",
    "}\n",
    "df['Program'] = df['Program'].str.lower().str.strip()\n",
    "df['Program'] = df['Program'].replace(program_mapping)\n",
    "\n",
    "print(df['Program'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_mapping = {\n",
    "    'meal delivery for seniors': 'homeless services',\n",
    "    'housing homeless assistance program (hhap) housing navigaiton/ casem management': 'flexible funds',\n",
    "    'd76 housing prevention and intervention program': 'housing assistance',\n",
    "    'vista homeless prevention and economic recovery project': 'flexible funds',\n",
    "    'gift cards': 'homeless services', \n",
    "    'regional task force homelessness meeting': 'homeless prevention',\n",
    "    'interim housing services for downtown chronically homeless': 'interim housing facility',\n",
    "}\n",
    "df['Program'] = df['Program'].str.lower().str.strip()\n",
    "df['Program'] = df['Program'].replace(program_mapping)\n",
    "\n",
    "print(df['Program'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_mapping = {\n",
    "    'womens resource center transitional housing': 'transitional housing and supportive services',\n",
    "    'fair housing': 'housing assistance', # not sure about this one\n",
    "    'clinical social worker': 'case management',\n",
    "    'program development': 'general funding for homelessness services',\n",
    "    'social worker program': 'case management',\n",
    "    'covid 19 homeless response full time caseworker': 'case management',\n",
    "    'acquisition of facilility for provision of homeless': 'housing assistance',\n",
    "}\n",
    "df['Program'] = df['Program'].str.lower().str.strip()\n",
    "df['Program'] = df['Program'].replace(program_mapping)\n",
    "\n",
    "print(df['Program'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_mapping = {\n",
    "    'housing navigator': 'housing navigation services',\n",
    "    'homeless prevention': 'homelessness prevention',\n",
    "}\n",
    "df['Program'] = df['Program'].str.lower().str.strip()\n",
    "df['Program'] = df['Program'].replace(program_mapping)\n",
    "\n",
    "print(df['Program'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/updated_dataset.csv', index=False)  # index=False prevents saving row numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `expenditures_and_PIT.csv`\n",
    "\n",
    "Input datasets: \n",
    "- `updated_dataset.csv`\n",
    "- `PITCount.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = pd.read_csv('../data/processed/updated_dataset.csv')\n",
    "exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NAs in the Date column\n",
    "exp = exp[exp['Date'].notna()]\n",
    "exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unique.ID', 'EndDate', 'AmendmentNumber', 'Funding.Agency', 'Funding.Source',\n",
    "       'Category', 'Location', 'Issued', 'Funding.Type', 'Years',\n",
    "       'Average.By.Year', 'City.Year', 'Population', 'Amount.Per.Capita',\n",
    "       'Amount.Per.PEH', 'Population.PEH']\n",
    "exp = exp.drop(columns=columns_to_drop)\n",
    "exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "exp = exp.rename(columns={\n",
    "    'Grantor': 'city', \n",
    "    'Grantee': 'grantee', \n",
    "    'Program': 'program', \n",
    "    'Year': 'year', \n",
    "    'Date': 'date', \n",
    "    'Amount': 'amount', \n",
    "    'ExpenditureType': 'exp_type'\n",
    "})\n",
    "\n",
    "# Try converting the 'date' column to datetime, allowing for mixed formats and coercing errors\n",
    "exp['date'] = pd.to_datetime(exp['date'], format='mixed', errors='coerce')\n",
    "\n",
    "# Check if any rows have invalid dates (i.e., rows where 'date' is NaT after coercion)\n",
    "invalid_dates = exp[exp['date'].isna()]\n",
    "\n",
    "# Add the \"month\" column with month names in word form\n",
    "exp['month'] = exp['date'].dt.strftime('%B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp['program'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanexp = pd.read_csv('../data/processed/updated_dataset.csv')\n",
    "cleanexp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanexp['Program'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_to_drop = ['point in time count','railing/fencing improvements','facility improvement','training/technical assistance']\n",
    "cleanexp = cleanexp[~cleanexp['Program'].isin(prog_to_drop)]\n",
    "cleanexp = cleanexp.dropna(subset=['Program'])\n",
    "cleanexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanexp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['Unique.ID', 'EndDate', 'AmendmentNumber', 'Funding.Agency', 'Funding.Source', 'Category','Location', 'Issued', 'Funding.Type', 'Years','Average.By.Year', 'City.Year', 'Population', 'Amount.Per.Capita','Amount.Per.PEH', 'Population.PEH']\n",
    "cleanexp = cleanexp.drop(columns=col_to_drop)\n",
    "cleanexp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanexp['Program'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "consolidation_map = {\n",
    "    'flexible funds': ['general funding for homelessness services', 'flexible funds'],\n",
    "    'transitional housing': ['temporary housing and services', 'transitional storage', 'transitional housing and supportive services', 'interim housing facility'],\n",
    "    'emergency shelter': ['shelter', 'emergency stabilization and supportive services'],\n",
    "    'staff and operations': ['211 assistance', 'case management', 'translation homeless plan mtg', 'cdbg consolidated plan consult', 'call center', 'employment/benefits']\n",
    "}\n",
    "\n",
    "cleanexp['Program'] = cleanexp['Program'].replace('emergency stabalization and supportive services', 'emergency shelter')\n",
    "\n",
    "def consolidate_program(program):\n",
    "    for consolidated_name, keywords in consolidation_map.items():\n",
    "        if any(keyword in program.lower() for keyword in keywords):\n",
    "            return consolidated_name\n",
    "    return program  # Return original if no match\n",
    "\n",
    "cleanexp['Program'] = cleanexp['Program'].apply(consolidate_program)\n",
    "cleanexp['Program'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging `updated_dataset.csv` with `PITCount.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIT = pd.read_csv('../data/raw/PITCount.csv')\n",
    "PIT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanexp = cleanexp.rename(columns={\n",
    "    'Grantor': 'City',\n",
    "    'Grantee': 'Grantee',\n",
    "    'Program': 'Program',\n",
    "    'Year': 'Year',\n",
    "    'Date': 'Date',\n",
    "    'Amount': 'Amount',\n",
    "    'ExpenditureType': 'ExpenditureType'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanexp['City'] = cleanexp['City'].str.replace('City of ', '')\n",
    "cleanexp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanexp = cleanexp[~cleanexp['Year'].isna()]\n",
    "cleanexp['Year'] = cleanexp['Year'].astype(int)\n",
    "cleanexp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanexp['City'] = cleanexp['City'].replace('SDHC', 'San Diego')\n",
    "PIT['Year'] -= 1 # Offset `Year` in `PIT` to have PIT assosiate with next \"Observation\" of PIT count of PEH\n",
    "df = pd.merge(PIT, cleanexp, on=['City', 'Year'], how='inner')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns= ['Latitude', 'Longitude', 'Date'])\n",
    "df['Amount'] = df['Amount'].str.replace('$', '', regex=False)\n",
    "df['Amount'] = df['Amount'].str.replace(',', '',regex=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = df['Year'].replace(0, np.nan)\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "df.to_csv('../data/processed/expenditures_and_PIT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pivoted_dataset.csv`\n",
    "Input datasets:\n",
    "- `expenditures_and_PIT.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/expenditures_and_pit.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by City, Year, and Program, then aggregate the total amount spent\n",
    "pivoted_df = df.groupby(['City', 'Year', 'Program'])['Amount'].sum().reset_index()\n",
    "\n",
    "pivoted_df = pivoted_df.pivot(index=['City', 'Year'], columns='Program', values='Amount').fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noticed there were a lot of zeroes as some programs only occur once or twice throughout the entire dataset\n",
    "pivoted_df.to_csv('../data/processed/pivoted_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pivoted_and_PIT.csv`\n",
    "Input datasets:\n",
    "- `pivoted_dataset.csv`\n",
    "- `PITCount.csv` (as cleaned in `expenditures_and_PIT.csv` cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = pd.read_csv('../data/processed/pivoted_dataset.csv')\n",
    "pivoted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIT['Year'] -= 1 # Year already offset `Year` in `PIT` to have PIT assosiate with next \"Observation\" of PIT count of PEH\n",
    "df = pd.merge(pivoted, PIT, on=['Year', 'City'], how='left')\n",
    "df = df.drop(columns= ['Latitude', 'Longitude'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/pivoted_and_PIT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pivoted_pit_grantee.csv`\n",
    "Input datasets:\n",
    "- `pivoted_and_PIT`\n",
    "- `expenditures_and_PIT.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df = pd.read_csv(\"../data/processed/pivoted_and_PIT.csv\")\n",
    "pivoted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\"../data/processed/expenditures_and_pit.csv\")\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging datasets to add 'Grantee' as a column on the pivoted dataset that contains pit count\n",
    "try:\n",
    "    merged_df = pivoted_df.merge(original_df[['City', 'Year', 'Grantee']].drop_duplicates(), on=['City', 'Year'], how='left')\n",
    "    print(\"\\nMerged Data:\")\n",
    "    print(merged_df.head())\n",
    "except TypeError as e:\n",
    "    print(\"TypeError:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving grantee over to the third row so its more accessible\n",
    "columns = merged_df.columns.tolist()\n",
    "\n",
    "columns.insert(2, columns.pop(columns.index('Grantee'))) \n",
    "merged_df = merged_df[columns] \n",
    "\n",
    "merged_df.to_csv('../data/processed/pivoted_pit_grantee.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement of Use of Generative AI\n",
    "\n",
    "During the preparation of this work the authors used ChatGPT as a coding assistant. After using this tool, the authors identified and reviewed the content as needed and takes full responsibility for the content of the code and resulting processed data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDCTA-SD-HPA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
